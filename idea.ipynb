{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import gc\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_lines(path, n_lines=None):\n",
    "    \"\"\"Creates a generator which reads and returns lines of\n",
    "    a json lines file, one line at a time, each as a dictionary.\n",
    "    \n",
    "    This could be used as a memory-efficient alternative of `pandas.read_json`\n",
    "    for reading a json lines file.\n",
    "    \"\"\"\n",
    "    with open(path, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if n_lines == i:\n",
    "                break\n",
    "            yield json.loads(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = os.path.join('output_data')\n",
    "\n",
    "aggregated_search_data_path = os.path.join(output_dir, 'aggregated_search_data.jsonl')\n",
    "preprocessed_products_path = os.path.join(output_dir, 'preprocessed_products.jsonl')\n",
    "preprocessed_test_queries_path = os.path.join(output_dir, 'preprocessed_test_queries.jsonl')\n",
    "\n",
    "# train_dat_file_path = os.path.join(output_dir, 'train.dat')\n",
    "\n",
    "# random_projection_mat_path = os.path.join(output_dir, 'random_projection_mat.npy')\n",
    "# product_features_path = os.path.join(output_dir, 'product_features.npy')\n",
    "# queries_train_features_path = os.path.join(output_dir, 'queries_train_features.npy')\n",
    "# queries_test_features_path = os.path.join(output_dir, 'queries_test_features.npy')\n",
    "# products_id_to_idx_path = os.path.join(output_dir, 'products_id_to_idx.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of tokens in the vocabulary of TF-IDF.\n",
    "VOCAB_SIZE = 4096\n",
    "# Embedding dimension used for random projection of TF-IDF vectors.\n",
    "EMBEDDING_DIM = 256\n",
    "# Number of training samples to use (set to None to use all samples).\n",
    "NUM_TRAIN_SAMPLES = 10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load aggregated search data which will be used as training data.\n",
    "aggregated_searches_df = pd.DataFrame(read_json_lines(aggregated_search_data_path, n_lines=NUM_TRAIN_SAMPLES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed product data.\n",
    "products_data_df = pd.DataFrame(read_json_lines(preprocessed_products_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed test queries.\n",
    "test_offline_queries_df = pd.DataFrame(read_json_lines(preprocessed_test_queries_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:01, 9028.72it/s]\n"
     ]
    }
   ],
   "source": [
    "limit = 200\n",
    "qids = []\n",
    "products = []\n",
    "target = []\n",
    "for qid, agg_search in tqdm(enumerate(aggregated_searches_df.itertuples(index=False))):\n",
    "    clicks = dict(zip(agg_search.clicks, agg_search.clicks_count))\n",
    "    for candidate_product_id in agg_search.results[:limit]:\n",
    "        if candidate_product_id is None:\n",
    "            continue\n",
    "        candidate_score = clicks.get(candidate_product_id, 0)\n",
    "        candidate_score = np.log2(candidate_score + 1)\n",
    "        \n",
    "        qids.append(qid)\n",
    "        products.append(candidate_product_id)\n",
    "        target.append(candidate_score)\n",
    "\n",
    "qids = np.array(qids)\n",
    "products = np.array(products)\n",
    "target = np.array(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'qid': qids, 'product': products, 'target': target})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "gss = GroupShuffleSplit(test_size=.30, n_splits=1, random_state=7).split(df, groups=df['qid'])\n",
    "\n",
    "X_train_inds, X_test_inds = next(gss)\n",
    "\n",
    "train_data= df.iloc[X_train_inds]\n",
    "X_train = train_data.loc[:, ~train_data.columns.isin(['qid','target'])]\n",
    "y_train = train_data.loc[:, train_data.columns.isin(['target'])]\n",
    "\n",
    "groups = train_data.groupby('qid').size().to_frame('size')['size'].to_numpy()\n",
    "\n",
    "test_data= df.iloc[X_test_inds]\n",
    "\n",
    "#We need to keep the id for later predictions\n",
    "X_test = test_data.loc[:, ~test_data.columns.isin(['target'])]\n",
    "y_test = test_data.loc[:, test_data.columns.isin(['target'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "model = xgb.XGBRanker(  \n",
    "    tree_method='gpu_hist',\n",
    "    booster='gbtree',\n",
    "    objective='rank:ndcg',\n",
    "    random_state=42, \n",
    "    learning_rate=0.1,\n",
    "    colsample_bytree=0.9, \n",
    "    eta=0.05, \n",
    "    max_depth=6, \n",
    "    n_estimators=110, \n",
    "    subsample=0.75 \n",
    "    )\n",
    "\n",
    "model.fit(X_train, y_train, group=groups, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "921c1c4932ebc6534d21dcf2780023961ea40e3e28695ae20f3dbb7777514cf0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
