{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_mappings = {\n",
    "    \"٥\": \"5\",\n",
    "    \"А\": \"a\",\n",
    "    \"В\": \"b\",\n",
    "    \"Е\": \"e\",\n",
    "    \"Н\": \"h\",\n",
    "    \"Р\": \"P\",\n",
    "    \"С\": \"C\",\n",
    "    \"Т\": \"T\",\n",
    "    \"а\": \"a\",\n",
    "    \"г\": \"r\",\n",
    "    \"е\": \"e\",\n",
    "    \"к\": \"k\",\n",
    "    \"м\": \"m\",\n",
    "    \"о\": \"o\",\n",
    "    \"р\": \"p\",\n",
    "    \"ڈ\": \"د\",\n",
    "    \"ڇ\": \"چ\",\n",
    "    # Persian numbers (will be raplaced by english one)\n",
    "    \"۰\": \"0\",\n",
    "    \"۱\": \"1\",\n",
    "    \"۲\": \"2\",\n",
    "    \"۳\": \"3\",\n",
    "    \"۴\": \"4\",\n",
    "    \"۵\": \"5\",\n",
    "    \"۶\": \"6\",\n",
    "    \"۷\": \"7\",\n",
    "    \"۸\": \"8\",\n",
    "    \"۹\": \"9\",\n",
    "    \".\": \".\",\n",
    "    # Arabic numbers (will be raplaced by english one)\n",
    "    \"٠\": \"0\",\n",
    "    \"١\": \"1\",\n",
    "    \"٢\": \"2\",\n",
    "    \"٣\": \"3\",\n",
    "    \"٤\": \"4\",\n",
    "    \"٥\": \"5\",\n",
    "    \"٦\": \"6\",\n",
    "    \"٧\": \"7\",\n",
    "    \"٨\": \"8\",\n",
    "    \"٩\": \"9\",\n",
    "    # Special Arabic Characters (will be replaced by persian one)\n",
    "    \"ك\": \"ک\",\n",
    "    \"ى\": \"ی\",\n",
    "    \"ي\": \"ی\",\n",
    "    \"ؤ\": \"و\",\n",
    "    \"ئ\": \"ی\",\n",
    "    \"إ\": \"ا\",\n",
    "    \"أ\": \"ا\",\n",
    "    \"آ\": \"ا\",\n",
    "    \"ة\": \"ه\",\n",
    "    \"ء\": \"ی\",\n",
    "    # French alphabet (will be raplaced by english one)\n",
    "    \"à\": \"a\",\n",
    "    \"ä\": \"a\",\n",
    "    \"ç\": \"c\",\n",
    "    \"é\": \"e\",\n",
    "    \"è\": \"e\",\n",
    "    \"ê\": \"e\",\n",
    "    \"ë\": \"e\",\n",
    "    \"î\": \"i\",\n",
    "    \"ï\": \"i\",\n",
    "    \"ô\": \"o\",\n",
    "    \"ù\": \"u\",\n",
    "    \"û\": \"u\",\n",
    "    \"ü\": \"u\",\n",
    "    # Camma (will be replaced by dots for floating point numbers)\n",
    "    \",\": \".\",\n",
    "    # And (will be replaced by dots for floating point numbers)\n",
    "    \"&\": \" and \",\n",
    "    # Vowels (will be removed)\n",
    "    \"ّ\": \"\",  # tashdid\n",
    "    \"َ\": \"\",  # a\n",
    "    \"ِ\": \"\",  # e\n",
    "    \"ُ\": \"\",  # o\n",
    "    \"ـ\": \"\",  # tatvil\n",
    "    # Spaces\n",
    "    \"‍\": \"\",  # 0x9E -> ZERO WIDTH JOINER\n",
    "    \"‌\": \" \",  # 0x9D -> ZERO WIDTH NON-JOINER\n",
    "    # Arabic Presentation Forms-A (will be replaced by persian one)\n",
    "    \"ﭐ\": \"ا\",\n",
    "    \"ﭑ\": \"ا\",\n",
    "    \"ﭖ\": \"پ\",\n",
    "    \"ﭗ\": \"پ\",\n",
    "    \"ﭘ\": \"پ\",\n",
    "    \"ﭙ\": \"پ\",\n",
    "    \"ﭞ\": \"ت\",\n",
    "    \"ﭟ\": \"ت\",\n",
    "    \"ﭠ\": \"ت\",\n",
    "    \"ﭡ\": \"ت\",\n",
    "    \"ﭺ\": \"چ\",\n",
    "    \"ﭻ\": \"چ\",\n",
    "    \"ﭼ\": \"چ\",\n",
    "    \"ﭽ\": \"چ\",\n",
    "    \"ﮊ\": \"ژ\",\n",
    "    \"ﮋ\": \"ژ\",\n",
    "    \"ﮎ\": \"ک\",\n",
    "    \"ﮏ\": \"ک\",\n",
    "    \"ﮐ\": \"ک\",\n",
    "    \"ﮑ\": \"ک\",\n",
    "    \"ﮒ\": \"گ\",\n",
    "    \"ﮓ\": \"گ\",\n",
    "    \"ﮔ\": \"گ\",\n",
    "    \"ﮕ\": \"گ\",\n",
    "    \"ﮤ\": \"ه\",\n",
    "    \"ﮥ\": \"ه\",\n",
    "    \"ﮦ\": \"ه\",\n",
    "    \"ﮪ\": \"ه\",\n",
    "    \"ﮫ\": \"ه\",\n",
    "    \"ﮬ\": \"ه\",\n",
    "    \"ﮭ\": \"ه\",\n",
    "    \"ﮮ\": \"ی\",\n",
    "    \"ﮯ\": \"ی\",\n",
    "    \"ﮰ\": \"ی\",\n",
    "    \"ﮱ\": \"ی\",\n",
    "    \"ﯼ\": \"ی\",\n",
    "    \"ﯽ\": \"ی\",\n",
    "    \"ﯾ\": \"ی\",\n",
    "    \"ﯿ\": \"ی\",\n",
    "    # Arabic Presentation Forms-B (will be removed)\n",
    "    \"ﹰ\": \"\",\n",
    "    \"ﹱ\": \"\",\n",
    "    \"ﹲ\": \"\",\n",
    "    \"ﹳ\": \"\",\n",
    "    \"ﹴ\": \"\",\n",
    "    \"﹵\": \"\",\n",
    "    \"ﹶ\": \"\",\n",
    "    \"ﹷ\": \"\",\n",
    "    \"ﹸ\": \"\",\n",
    "    \"ﹹ\": \"\",\n",
    "    \"ﹺ\": \"\",\n",
    "    \"ﹻ\": \"\",\n",
    "    \"ﹼ\": \"\",\n",
    "    \"ﹽ\": \"\",\n",
    "    \"ﹾ\": \"\",\n",
    "    \"ﹿ\": \"\",\n",
    "    # Arabic Presentation Forms-B (will be replaced by persian one)\n",
    "    \"ﺀ\": \"ی\",\n",
    "    \"ﺁ\": \"ا\",\n",
    "    \"ﺂ\": \"ا\",\n",
    "    \"ﺃ\": \"ا\",\n",
    "    \"ﺄ\": \"ا\",\n",
    "    \"ﺅ\": \"و\",\n",
    "    \"ﺆ\": \"و\",\n",
    "    \"ﺇ\": \"ا\",\n",
    "    \"ﺈ\": \"ا\",\n",
    "    \"ﺉ\": \"ی\",\n",
    "    \"ﺊ\": \"ی\",\n",
    "    \"ﺋ\": \"ی\",\n",
    "    \"ﺌ\": \"ی\",\n",
    "    \"ﺍ\": \"ا\",\n",
    "    \"ﺎ\": \"ا\",\n",
    "    \"ﺏ\": \"ب\",\n",
    "    \"ﺐ\": \"ب\",\n",
    "    \"ﺑ\": \"ب\",\n",
    "    \"ﺒ\": \"ب\",\n",
    "    \"ﺓ\": \"ه\",\n",
    "    \"ﺔ\": \"ه\",\n",
    "    \"ﺕ\": \"ت\",\n",
    "    \"ﺖ\": \"ت\",\n",
    "    \"ﺗ\": \"ت\",\n",
    "    \"ﺘ\": \"ت\",\n",
    "    \"ﺙ\": \"ث\",\n",
    "    \"ﺚ\": \"ث\",\n",
    "    \"ﺛ\": \"ث\",\n",
    "    \"ﺜ\": \"ث\",\n",
    "    \"ﺝ\": \"ج\",\n",
    "    \"ﺞ\": \"ج\",\n",
    "    \"ﺟ\": \"ج\",\n",
    "    \"ﺠ\": \"ج\",\n",
    "    \"ﺡ\": \"ح\",\n",
    "    \"ﺢ\": \"ح\",\n",
    "    \"ﺣ\": \"ح\",\n",
    "    \"ﺤ\": \"ح\",\n",
    "    \"ﺥ\": \"خ\",\n",
    "    \"ﺦ\": \"خ\",\n",
    "    \"ﺧ\": \"خ\",\n",
    "    \"ﺨ\": \"خ\",\n",
    "    \"ﺩ\": \"د\",\n",
    "    \"ﺪ\": \"د\",\n",
    "    \"ﺫ\": \"ذ\",\n",
    "    \"ﺬ\": \"ذ\",\n",
    "    \"ﺭ\": \"ر\",\n",
    "    \"ﺮ\": \"ر\",\n",
    "    \"ﺯ\": \"ز\",\n",
    "    \"ﺰ\": \"ز\",\n",
    "    \"ﺱ\": \"س\",\n",
    "    \"ﺲ\": \"س\",\n",
    "    \"ﺳ\": \"س\",\n",
    "    \"ﺴ\": \"س\",\n",
    "    \"ﺵ\": \"ش\",\n",
    "    \"ﺶ\": \"ش\",\n",
    "    \"ﺷ\": \"ش\",\n",
    "    \"ﺸ\": \"ش\",\n",
    "    \"ﺹ\": \"ص\",\n",
    "    \"ﺺ\": \"ص\",\n",
    "    \"ﺻ\": \"ص\",\n",
    "    \"ﺼ\": \"ص\",\n",
    "    \"ﺽ\": \"ض\",\n",
    "    \"ﺾ\": \"ض\",\n",
    "    \"ﺿ\": \"ض\",\n",
    "    \"ﻀ\": \"ض\",\n",
    "    \"ﻁ\": \"ط\",\n",
    "    \"ﻂ\": \"ط\",\n",
    "    \"ﻃ\": \"ط\",\n",
    "    \"ﻄ\": \"ط\",\n",
    "    \"ﻅ\": \"ظ\",\n",
    "    \"ﻆ\": \"ظ\",\n",
    "    \"ﻇ\": \"ظ\",\n",
    "    \"ﻈ\": \"ظ\",\n",
    "    \"ﻉ\": \"ع\",\n",
    "    \"ﻊ\": \"ع\",\n",
    "    \"ﻋ\": \"ع\",\n",
    "    \"ﻌ\": \"ع\",\n",
    "    \"ﻍ\": \"غ\",\n",
    "    \"ﻎ\": \"غ\",\n",
    "    \"ﻏ\": \"غ\",\n",
    "    \"ﻐ\": \"غ\",\n",
    "    \"ﻑ\": \"ف\",\n",
    "    \"ﻒ\": \"ف\",\n",
    "    \"ﻓ\": \"ف\",\n",
    "    \"ﻔ\": \"ف\",\n",
    "    \"ﻕ\": \"ق\",\n",
    "    \"ﻖ\": \"ق\",\n",
    "    \"ﻗ\": \"ق\",\n",
    "    \"ﻘ\": \"ق\",\n",
    "    \"ﻙ\": \"ک\",\n",
    "    \"ﻚ\": \"ک\",\n",
    "    \"ﻛ\": \"ک\",\n",
    "    \"ﻜ\": \"ک\",\n",
    "    \"ﻝ\": \"ل\",\n",
    "    \"ﻞ\": \"ل\",\n",
    "    \"ﻟ\": \"ل\",\n",
    "    \"ﻠ\": \"ل\",\n",
    "    \"ﻡ\": \"م\",\n",
    "    \"ﻢ\": \"م\",\n",
    "    \"ﻣ\": \"م\",\n",
    "    \"ﻤ\": \"م\",\n",
    "    \"ﻥ\": \"ن\",\n",
    "    \"ﻦ\": \"ن\",\n",
    "    \"ﻧ\": \"ن\",\n",
    "    \"ﻨ\": \"ن\",\n",
    "    \"ﻩ\": \"ه\",\n",
    "    \"ﻪ\": \"ه\",\n",
    "    \"ﻫ\": \"ه\",\n",
    "    \"ﻬ\": \"ه\",\n",
    "    \"ﻭ\": \"و\",\n",
    "    \"ﻮ\": \"و\",\n",
    "    \"ﻯ\": \"ی\",\n",
    "    \"ﻰ\": \"ی\",\n",
    "    \"ﻱ\": \"ی\",\n",
    "    \"ﻲ\": \"ی\",\n",
    "    \"ﻳ\": \"ی\",\n",
    "    \"ﻴ\": \"ی\",\n",
    "    \"ﻵ\": \"لا\",\n",
    "    \"ﻶ\": \"لا\",\n",
    "    \"ﻷ\": \"لا\",\n",
    "    \"ﻸ\": \"لا\",\n",
    "    \"ﻹ\": \"لا\",\n",
    "    \"ﻺ\": \"لا\",\n",
    "    \"ﻻ\": \"لا\",\n",
    "    \"ﻼ\": \"لا\",\n",
    "}\n",
    "\n",
    "valid_chars = [\n",
    "    \" \",\n",
    "    \"0\",\n",
    "    \"1\",\n",
    "    \"2\",\n",
    "    \"3\",\n",
    "    \"4\",\n",
    "    \"5\",\n",
    "    \"6\",\n",
    "    \"7\",\n",
    "    \"8\",\n",
    "    \"9\",\n",
    "    \"A\",\n",
    "    \"B\",\n",
    "    \"C\",\n",
    "    \"D\",\n",
    "    \"E\",\n",
    "    \"F\",\n",
    "    \"G\",\n",
    "    \"H\",\n",
    "    \"I\",\n",
    "    \"J\",\n",
    "    \"K\",\n",
    "    \"L\",\n",
    "    \"M\",\n",
    "    \"N\",\n",
    "    \"O\",\n",
    "    \"P\",\n",
    "    \"Q\",\n",
    "    \"R\",\n",
    "    \"S\",\n",
    "    \"T\",\n",
    "    \"U\",\n",
    "    \"V\",\n",
    "    \"W\",\n",
    "    \"X\",\n",
    "    \"Y\",\n",
    "    \"Z\",\n",
    "    \"a\",\n",
    "    \"b\",\n",
    "    \"c\",\n",
    "    \"d\",\n",
    "    \"e\",\n",
    "    \"f\",\n",
    "    \"g\",\n",
    "    \"h\",\n",
    "    \"i\",\n",
    "    \"j\",\n",
    "    \"k\",\n",
    "    \"l\",\n",
    "    \"m\",\n",
    "    \"n\",\n",
    "    \"o\",\n",
    "    \"p\",\n",
    "    \"q\",\n",
    "    \"r\",\n",
    "    \"s\",\n",
    "    \"t\",\n",
    "    \"u\",\n",
    "    \"v\",\n",
    "    \"w\",\n",
    "    \"x\",\n",
    "    \"y\",\n",
    "    \"z\",\n",
    "    \"ا\",\n",
    "    \"ب\",\n",
    "    \"ت\",\n",
    "    \"ث\",\n",
    "    \"ج\",\n",
    "    \"ح\",\n",
    "    \"خ\",\n",
    "    \"د\",\n",
    "    \"ذ\",\n",
    "    \"ر\",\n",
    "    \"ز\",\n",
    "    \"س\",\n",
    "    \"ش\",\n",
    "    \"ص\",\n",
    "    \"ض\",\n",
    "    \"ط\",\n",
    "    \"ظ\",\n",
    "    \"ع\",\n",
    "    \"غ\",\n",
    "    \"ف\",\n",
    "    \"ق\",\n",
    "    \"ل\",\n",
    "    \"م\",\n",
    "    \"ن\",\n",
    "    \"ه\",\n",
    "    \"و\",\n",
    "    \"پ\",\n",
    "    \"چ\",\n",
    "    \"ژ\",\n",
    "    \"ک\",\n",
    "    \"گ\",\n",
    "    \"ی\",\n",
    "]\n",
    "\n",
    "translation_table = dict((ord(a), b) for a, b in char_mappings.items())\n",
    "\n",
    "# Create a regex for recognizing invalid characters.\n",
    "nonvalid_reg_text = '[^{}]'.format(\"\".join(valid_chars))\n",
    "nonvalid_reg = re.compile(nonvalid_reg_text)\n",
    "\n",
    "\n",
    "def normalize_text(text, to_lower=True, remove_invalid=True):\n",
    "    # Map invalid characters with replacement to valid characters.\n",
    "    text = text.translate(translation_table)\n",
    "    if to_lower:\n",
    "        text = text.lower()\n",
    "    if remove_invalid:\n",
    "        text = nonvalid_reg.sub(' ', text)\n",
    "    # Replace consecutive whitespaces with a single space character.\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_lines(path, n_lines=None):\n",
    "    \"\"\"Creates a generator which reads and returns lines of\n",
    "    a json lines file, one line at a time, each as a dictionary.\n",
    "    \n",
    "    This could be used as a memory-efficient alternative of `pandas.read_json`\n",
    "    for reading a json lines file.\n",
    "    \"\"\"\n",
    "    with open(path, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if n_lines == i:\n",
    "                break\n",
    "            yield json.loads(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JSONLinesWriter:\n",
    "    \"\"\"\n",
    "    Helper class to write list of dictionaries into a file in json lines\n",
    "    format, i.e. one json record per line.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_path):\n",
    "        self.fd = None\n",
    "        self.file_path = file_path\n",
    "        self.delimiter = \"\\n\"\n",
    "\n",
    "    def open(self):\n",
    "        self.fd = open(self.file_path, \"w\")\n",
    "        self.first_record_written = False\n",
    "        return self\n",
    "\n",
    "    def close(self):\n",
    "        self.fd.close()\n",
    "        self.fd = None\n",
    "\n",
    "    def write_record(self, obj):\n",
    "        if self.first_record_written:\n",
    "            self.fd.write(self.delimiter)\n",
    "        self.fd.write(json.dumps(obj))\n",
    "        self.first_record_written = True\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self.open()\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        self.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join('data')\n",
    "output_dir = os.path.join('output_data')\n",
    "\n",
    "search_data_path = os.path.join(data_dir, 'torob-search-data_v1.jsonl')\n",
    "aggregated_search_data_path = os.path.join(output_dir, 'aggregated_search_data.jsonl')\n",
    "\n",
    "products_path = os.path.join(data_dir, 'products-info_v1.jsonl')\n",
    "preprocessed_products_path = os.path.join(output_dir, 'preprocessed_products.jsonl')\n",
    "\n",
    "test_data_path = os.path.join(data_dir, 'test-offline-data_v1.jsonl')\n",
    "preprocessed_test_queries_path = os.path.join(output_dir, 'preprocessed_test_queries.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_searches(search_data_path, output_path):\n",
    "    \"\"\"Aggregate searches based on raw query.\n",
    "    \n",
    "    For each unique raw query in the search data, the frequency of products and\n",
    "    clicked products would be aggregated.\n",
    "    \"\"\"\n",
    "    agg_searches = defaultdict(\n",
    "        lambda : dict(\n",
    "            results=Counter(),\n",
    "            clicks=Counter(),\n",
    "        )\n",
    "    )\n",
    "    print(\"Aggregating searches based on raw query...\")\n",
    "    for search in tqdm(read_json_lines(search_data_path)):\n",
    "        agg_searches[search['raw_query']]['results'].update(search['result'])\n",
    "        agg_searches[search['raw_query']]['clicks'].update(search['clicked_result'])\n",
    "    \n",
    "    print('Writing aggregated searches into file...')\n",
    "    with JSONLinesWriter(output_path) as out_file:\n",
    "        for raw_query, stats in tqdm(agg_searches.items()):\n",
    "            results, results_count = list(zip(*stats['results'].most_common()))\n",
    "            clicks, clicks_count = list(zip(*stats['clicks'].most_common()))\n",
    "            record = {\n",
    "                'raw_query': raw_query,\n",
    "                'raw_query_normalized': normalize_text(raw_query),\n",
    "                'results': results,\n",
    "                'results_count': results_count,\n",
    "                'clicks': clicks,\n",
    "                'clicks_count': clicks_count,\n",
    "            }\n",
    "            out_file.write_record(record)\n",
    "\n",
    "    print(\"Finished aggregating searches.\")\n",
    "    print(f'Number of aggregate search records: {len(agg_searches)}')\n",
    "    print(f\"The aggregated searches data were stored in '{output_path}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating searches based on raw query...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2499901it [42:14, 986.41it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing aggregated searches into file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 34261/270099 [00:02<00:11, 20047.74it/s]"
     ]
    }
   ],
   "source": [
    "aggregate_searches(search_data_path, aggregated_search_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_products(products_path, output_path):\n",
    "    \"\"\"Preprocess product names.\n",
    "    \n",
    "    The different titles of a product are concatenated together and \n",
    "    the resulting string would be normalized. Then, the normalized title\n",
    "    is split into tokens and only the set of unique tokens would be selected\n",
    "    as the final title of the product.\n",
    "    \"\"\"\n",
    "    print('Preprocessing products...')\n",
    "    count = 0\n",
    "    with JSONLinesWriter(output_path) as out_file:\n",
    "        for product in tqdm(read_json_lines(products_path)):\n",
    "            titles = product['titles']\n",
    "            titles_concat_normalized = normalize_text(\" \".join(titles))\n",
    "            titles_words_set = set(titles_concat_normalized.split())\n",
    "            titles_words_concat = \" \".join(titles_words_set)\n",
    "            \n",
    "            record = {\n",
    "                'id': product['id'],\n",
    "                'title_normalized': titles_words_concat,\n",
    "                'category_name': product['category_name'],\n",
    "                'min_price': product['min_price'],\n",
    "                'max_price': product['max_price'],\n",
    "                'avg_price': product['avg_price'],\n",
    "                'min_num_shops': product['min_num_shops'],\n",
    "                'max_num_shops': product['max_num_shops'],\n",
    "                'avg_num_shops': product['avg_num_shops'],\n",
    "            }\n",
    "            out_file.write_record(record)\n",
    "            count += 1\n",
    "    print('Finished preprocessing products.')\n",
    "    print(f'Number of processed products: {count}')\n",
    "    print(f\"The processed products data were stored in '{output_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing products...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3612277it [01:07, 53632.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished preprocessing products.\n",
      "Number of processed products: 3612277\n",
      "The processed products data were stored in 'output_data/preprocessed_products.jsonl'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preprocess_products(products_path, preprocessed_products_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_test_queries(test_data_path, output_path):\n",
    "    \"\"\"Normalize test queries.\"\"\"\n",
    "    print('Preprocessing test queries...')\n",
    "    count = 0\n",
    "    with JSONLinesWriter(output_path) as out_file:\n",
    "        for test_sample in tqdm(read_json_lines(test_data_path)):\n",
    "            normalized_query = normalize_text(test_sample['raw_query'])\n",
    "            record = {\n",
    "                'raw_query_normalized': normalized_query,\n",
    "            }\n",
    "            count += 1\n",
    "            out_file.write_record(record)\n",
    "    print('Finished preprocessing test queries.')\n",
    "    print(f'Number of processed test queries: {count}')\n",
    "    print(f\"The processed test queries were stored in '{output_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing test queries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23140it [00:00, 113712.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished preprocessing test queries.\n",
      "Number of processed test queries: 23140\n",
      "The processed test queries were stored in 'output/preprocessed_test_queries.jsonl'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preprocess_test_queries(test_data_path, preprocessed_test_queries_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f0cf53528eda0cb4bf223c9d630c850e13d0e5d45b6f3ca080b7ff6c56946f6a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
