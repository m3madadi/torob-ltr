{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AntCuf_VyqaC"
      },
      "source": [
        "# Baseline Solution for Torob Data Challenge 2023\n",
        "\n",
        "**Congratulations for participating in Torob Data Challenge 2023! 🎉**\n",
        "\n",
        "In this notebook, a baseline solution is provied for solving the challenge. As its name suggests, this solution only serves as a *baseline* which helps you to get a better idea of how the challenge data can be loaded and processed; and also, it can be potentially used as a starting point which you can add more complexity upon it and do more tuning of the parameters to get a better result. Of course, the approach/solution provided here is only one of the many different approaches for solving learning-to-rank problems and you are not required to use this solution at all; although, we strongly suggest that you should at least read and run it once to better understand the challenge as well as its data.\n",
        "\n",
        "In this baseline solution, a LambdaMART model is trained to rank products based on their relevance. We use a combination of TF-IDF and random projection to represent queries as well as product names. As mentioned above, this solution is not tuned to get the best performance possible and there is a lot of room for improvements which you can try and experiment with. Also, feel free to try other solutions which are based on entirely different models, representation, etc. than this baseline. We are really excited and looking forward to see you and your novel solutions in the challenge!\n",
        "\n",
        "Ok, let's get started!\n",
        "\n",
        "NOTE: you can find a brief description of the baseline solution in the following document.\n",
        "https://docs.google.com/document/d/1aLvD5RoakD-eS7IXcTMGSfo4PHzxa75dZwsC1uhFQXw/edit?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tysGdBxBC11n"
      },
      "outputs": [],
      "source": [
        "# !pip install --no-cache-dir --upgrade gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OscMtb9Iy4Zr"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict, Counter\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "\n",
        "from tqdm import tqdm\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06T82sDey_R5"
      },
      "source": [
        "## Fetch Data\n",
        "\n",
        "Fetch the public challenge data from Google Drive and extract its content to a directory called `data`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyI_Jujoy-z1"
      },
      "outputs": [],
      "source": [
        "# !gdown 1spSFY1yieMcjGJc989bbbEheNhegmEmR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0djFH08UzEIL"
      },
      "outputs": [],
      "source": [
        "# !7z e torob-data-challenge-2023_datafiles_v1.7z -odata/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMuNEV44zGJ8"
      },
      "outputs": [],
      "source": [
        "# Create a directory for all the generated files during execution of notebook.\n",
        "!mkdir output_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XfL-6e5zeLs"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8d96ac2b-c4c3-4e56-9bdd-5772c21c7099"
      },
      "outputs": [],
      "source": [
        "char_mappings = {\n",
        "    \"٥\": \"5\",\n",
        "    \"А\": \"a\",\n",
        "    \"В\": \"b\",\n",
        "    \"Е\": \"e\",\n",
        "    \"Н\": \"h\",\n",
        "    \"Р\": \"P\",\n",
        "    \"С\": \"C\",\n",
        "    \"Т\": \"T\",\n",
        "    \"а\": \"a\",\n",
        "    \"г\": \"r\",\n",
        "    \"е\": \"e\",\n",
        "    \"к\": \"k\",\n",
        "    \"м\": \"m\",\n",
        "    \"о\": \"o\",\n",
        "    \"р\": \"p\",\n",
        "    \"ڈ\": \"د\",\n",
        "    \"ڇ\": \"چ\",\n",
        "    # Persian numbers (will be raplaced by english one)\n",
        "    \"۰\": \"0\",\n",
        "    \"۱\": \"1\",\n",
        "    \"۲\": \"2\",\n",
        "    \"۳\": \"3\",\n",
        "    \"۴\": \"4\",\n",
        "    \"۵\": \"5\",\n",
        "    \"۶\": \"6\",\n",
        "    \"۷\": \"7\",\n",
        "    \"۸\": \"8\",\n",
        "    \"۹\": \"9\",\n",
        "    \".\": \".\",\n",
        "    # Arabic numbers (will be raplaced by english one)\n",
        "    \"٠\": \"0\",\n",
        "    \"١\": \"1\",\n",
        "    \"٢\": \"2\",\n",
        "    \"٣\": \"3\",\n",
        "    \"٤\": \"4\",\n",
        "    \"٥\": \"5\",\n",
        "    \"٦\": \"6\",\n",
        "    \"٧\": \"7\",\n",
        "    \"٨\": \"8\",\n",
        "    \"٩\": \"9\",\n",
        "    # Special Arabic Characters (will be replaced by persian one)\n",
        "    \"ك\": \"ک\",\n",
        "    \"ى\": \"ی\",\n",
        "    \"ي\": \"ی\",\n",
        "    \"ؤ\": \"و\",\n",
        "    \"ئ\": \"ی\",\n",
        "    \"إ\": \"ا\",\n",
        "    \"أ\": \"ا\",\n",
        "    \"آ\": \"ا\",\n",
        "    \"ة\": \"ه\",\n",
        "    \"ء\": \"ی\",\n",
        "    # French alphabet (will be raplaced by english one)\n",
        "    \"à\": \"a\",\n",
        "    \"ä\": \"a\",\n",
        "    \"ç\": \"c\",\n",
        "    \"é\": \"e\",\n",
        "    \"è\": \"e\",\n",
        "    \"ê\": \"e\",\n",
        "    \"ë\": \"e\",\n",
        "    \"î\": \"i\",\n",
        "    \"ï\": \"i\",\n",
        "    \"ô\": \"o\",\n",
        "    \"ù\": \"u\",\n",
        "    \"û\": \"u\",\n",
        "    \"ü\": \"u\",\n",
        "    # Camma (will be replaced by dots for floating point numbers)\n",
        "    \",\": \".\",\n",
        "    # And (will be replaced by dots for floating point numbers)\n",
        "    \"&\": \" and \",\n",
        "    # Vowels (will be removed)\n",
        "    \"ّ\": \"\",  # tashdid\n",
        "    \"َ\": \"\",  # a\n",
        "    \"ِ\": \"\",  # e\n",
        "    \"ُ\": \"\",  # o\n",
        "    \"ـ\": \"\",  # tatvil\n",
        "    # Spaces\n",
        "    \"‍\": \"\",  # 0x9E -> ZERO WIDTH JOINER\n",
        "    \"‌\": \" \",  # 0x9D -> ZERO WIDTH NON-JOINER\n",
        "    # Arabic Presentation Forms-A (will be replaced by persian one)\n",
        "    \"ﭐ\": \"ا\",\n",
        "    \"ﭑ\": \"ا\",\n",
        "    \"ﭖ\": \"پ\",\n",
        "    \"ﭗ\": \"پ\",\n",
        "    \"ﭘ\": \"پ\",\n",
        "    \"ﭙ\": \"پ\",\n",
        "    \"ﭞ\": \"ت\",\n",
        "    \"ﭟ\": \"ت\",\n",
        "    \"ﭠ\": \"ت\",\n",
        "    \"ﭡ\": \"ت\",\n",
        "    \"ﭺ\": \"چ\",\n",
        "    \"ﭻ\": \"چ\",\n",
        "    \"ﭼ\": \"چ\",\n",
        "    \"ﭽ\": \"چ\",\n",
        "    \"ﮊ\": \"ژ\",\n",
        "    \"ﮋ\": \"ژ\",\n",
        "    \"ﮎ\": \"ک\",\n",
        "    \"ﮏ\": \"ک\",\n",
        "    \"ﮐ\": \"ک\",\n",
        "    \"ﮑ\": \"ک\",\n",
        "    \"ﮒ\": \"گ\",\n",
        "    \"ﮓ\": \"گ\",\n",
        "    \"ﮔ\": \"گ\",\n",
        "    \"ﮕ\": \"گ\",\n",
        "    \"ﮤ\": \"ه\",\n",
        "    \"ﮥ\": \"ه\",\n",
        "    \"ﮦ\": \"ه\",\n",
        "    \"ﮪ\": \"ه\",\n",
        "    \"ﮫ\": \"ه\",\n",
        "    \"ﮬ\": \"ه\",\n",
        "    \"ﮭ\": \"ه\",\n",
        "    \"ﮮ\": \"ی\",\n",
        "    \"ﮯ\": \"ی\",\n",
        "    \"ﮰ\": \"ی\",\n",
        "    \"ﮱ\": \"ی\",\n",
        "    \"ﯼ\": \"ی\",\n",
        "    \"ﯽ\": \"ی\",\n",
        "    \"ﯾ\": \"ی\",\n",
        "    \"ﯿ\": \"ی\",\n",
        "    # Arabic Presentation Forms-B (will be removed)\n",
        "    \"ﹰ\": \"\",\n",
        "    \"ﹱ\": \"\",\n",
        "    \"ﹲ\": \"\",\n",
        "    \"ﹳ\": \"\",\n",
        "    \"ﹴ\": \"\",\n",
        "    \"﹵\": \"\",\n",
        "    \"ﹶ\": \"\",\n",
        "    \"ﹷ\": \"\",\n",
        "    \"ﹸ\": \"\",\n",
        "    \"ﹹ\": \"\",\n",
        "    \"ﹺ\": \"\",\n",
        "    \"ﹻ\": \"\",\n",
        "    \"ﹼ\": \"\",\n",
        "    \"ﹽ\": \"\",\n",
        "    \"ﹾ\": \"\",\n",
        "    \"ﹿ\": \"\",\n",
        "    # Arabic Presentation Forms-B (will be replaced by persian one)\n",
        "    \"ﺀ\": \"ی\",\n",
        "    \"ﺁ\": \"ا\",\n",
        "    \"ﺂ\": \"ا\",\n",
        "    \"ﺃ\": \"ا\",\n",
        "    \"ﺄ\": \"ا\",\n",
        "    \"ﺅ\": \"و\",\n",
        "    \"ﺆ\": \"و\",\n",
        "    \"ﺇ\": \"ا\",\n",
        "    \"ﺈ\": \"ا\",\n",
        "    \"ﺉ\": \"ی\",\n",
        "    \"ﺊ\": \"ی\",\n",
        "    \"ﺋ\": \"ی\",\n",
        "    \"ﺌ\": \"ی\",\n",
        "    \"ﺍ\": \"ا\",\n",
        "    \"ﺎ\": \"ا\",\n",
        "    \"ﺏ\": \"ب\",\n",
        "    \"ﺐ\": \"ب\",\n",
        "    \"ﺑ\": \"ب\",\n",
        "    \"ﺒ\": \"ب\",\n",
        "    \"ﺓ\": \"ه\",\n",
        "    \"ﺔ\": \"ه\",\n",
        "    \"ﺕ\": \"ت\",\n",
        "    \"ﺖ\": \"ت\",\n",
        "    \"ﺗ\": \"ت\",\n",
        "    \"ﺘ\": \"ت\",\n",
        "    \"ﺙ\": \"ث\",\n",
        "    \"ﺚ\": \"ث\",\n",
        "    \"ﺛ\": \"ث\",\n",
        "    \"ﺜ\": \"ث\",\n",
        "    \"ﺝ\": \"ج\",\n",
        "    \"ﺞ\": \"ج\",\n",
        "    \"ﺟ\": \"ج\",\n",
        "    \"ﺠ\": \"ج\",\n",
        "    \"ﺡ\": \"ح\",\n",
        "    \"ﺢ\": \"ح\",\n",
        "    \"ﺣ\": \"ح\",\n",
        "    \"ﺤ\": \"ح\",\n",
        "    \"ﺥ\": \"خ\",\n",
        "    \"ﺦ\": \"خ\",\n",
        "    \"ﺧ\": \"خ\",\n",
        "    \"ﺨ\": \"خ\",\n",
        "    \"ﺩ\": \"د\",\n",
        "    \"ﺪ\": \"د\",\n",
        "    \"ﺫ\": \"ذ\",\n",
        "    \"ﺬ\": \"ذ\",\n",
        "    \"ﺭ\": \"ر\",\n",
        "    \"ﺮ\": \"ر\",\n",
        "    \"ﺯ\": \"ز\",\n",
        "    \"ﺰ\": \"ز\",\n",
        "    \"ﺱ\": \"س\",\n",
        "    \"ﺲ\": \"س\",\n",
        "    \"ﺳ\": \"س\",\n",
        "    \"ﺴ\": \"س\",\n",
        "    \"ﺵ\": \"ش\",\n",
        "    \"ﺶ\": \"ش\",\n",
        "    \"ﺷ\": \"ش\",\n",
        "    \"ﺸ\": \"ش\",\n",
        "    \"ﺹ\": \"ص\",\n",
        "    \"ﺺ\": \"ص\",\n",
        "    \"ﺻ\": \"ص\",\n",
        "    \"ﺼ\": \"ص\",\n",
        "    \"ﺽ\": \"ض\",\n",
        "    \"ﺾ\": \"ض\",\n",
        "    \"ﺿ\": \"ض\",\n",
        "    \"ﻀ\": \"ض\",\n",
        "    \"ﻁ\": \"ط\",\n",
        "    \"ﻂ\": \"ط\",\n",
        "    \"ﻃ\": \"ط\",\n",
        "    \"ﻄ\": \"ط\",\n",
        "    \"ﻅ\": \"ظ\",\n",
        "    \"ﻆ\": \"ظ\",\n",
        "    \"ﻇ\": \"ظ\",\n",
        "    \"ﻈ\": \"ظ\",\n",
        "    \"ﻉ\": \"ع\",\n",
        "    \"ﻊ\": \"ع\",\n",
        "    \"ﻋ\": \"ع\",\n",
        "    \"ﻌ\": \"ع\",\n",
        "    \"ﻍ\": \"غ\",\n",
        "    \"ﻎ\": \"غ\",\n",
        "    \"ﻏ\": \"غ\",\n",
        "    \"ﻐ\": \"غ\",\n",
        "    \"ﻑ\": \"ف\",\n",
        "    \"ﻒ\": \"ف\",\n",
        "    \"ﻓ\": \"ف\",\n",
        "    \"ﻔ\": \"ف\",\n",
        "    \"ﻕ\": \"ق\",\n",
        "    \"ﻖ\": \"ق\",\n",
        "    \"ﻗ\": \"ق\",\n",
        "    \"ﻘ\": \"ق\",\n",
        "    \"ﻙ\": \"ک\",\n",
        "    \"ﻚ\": \"ک\",\n",
        "    \"ﻛ\": \"ک\",\n",
        "    \"ﻜ\": \"ک\",\n",
        "    \"ﻝ\": \"ل\",\n",
        "    \"ﻞ\": \"ل\",\n",
        "    \"ﻟ\": \"ل\",\n",
        "    \"ﻠ\": \"ل\",\n",
        "    \"ﻡ\": \"م\",\n",
        "    \"ﻢ\": \"م\",\n",
        "    \"ﻣ\": \"م\",\n",
        "    \"ﻤ\": \"م\",\n",
        "    \"ﻥ\": \"ن\",\n",
        "    \"ﻦ\": \"ن\",\n",
        "    \"ﻧ\": \"ن\",\n",
        "    \"ﻨ\": \"ن\",\n",
        "    \"ﻩ\": \"ه\",\n",
        "    \"ﻪ\": \"ه\",\n",
        "    \"ﻫ\": \"ه\",\n",
        "    \"ﻬ\": \"ه\",\n",
        "    \"ﻭ\": \"و\",\n",
        "    \"ﻮ\": \"و\",\n",
        "    \"ﻯ\": \"ی\",\n",
        "    \"ﻰ\": \"ی\",\n",
        "    \"ﻱ\": \"ی\",\n",
        "    \"ﻲ\": \"ی\",\n",
        "    \"ﻳ\": \"ی\",\n",
        "    \"ﻴ\": \"ی\",\n",
        "    \"ﻵ\": \"لا\",\n",
        "    \"ﻶ\": \"لا\",\n",
        "    \"ﻷ\": \"لا\",\n",
        "    \"ﻸ\": \"لا\",\n",
        "    \"ﻹ\": \"لا\",\n",
        "    \"ﻺ\": \"لا\",\n",
        "    \"ﻻ\": \"لا\",\n",
        "    \"ﻼ\": \"لا\",\n",
        "}\n",
        "\n",
        "valid_chars = [\n",
        "    \" \",\n",
        "    \"0\",\n",
        "    \"1\",\n",
        "    \"2\",\n",
        "    \"3\",\n",
        "    \"4\",\n",
        "    \"5\",\n",
        "    \"6\",\n",
        "    \"7\",\n",
        "    \"8\",\n",
        "    \"9\",\n",
        "    \"A\",\n",
        "    \"B\",\n",
        "    \"C\",\n",
        "    \"D\",\n",
        "    \"E\",\n",
        "    \"F\",\n",
        "    \"G\",\n",
        "    \"H\",\n",
        "    \"I\",\n",
        "    \"J\",\n",
        "    \"K\",\n",
        "    \"L\",\n",
        "    \"M\",\n",
        "    \"N\",\n",
        "    \"O\",\n",
        "    \"P\",\n",
        "    \"Q\",\n",
        "    \"R\",\n",
        "    \"S\",\n",
        "    \"T\",\n",
        "    \"U\",\n",
        "    \"V\",\n",
        "    \"W\",\n",
        "    \"X\",\n",
        "    \"Y\",\n",
        "    \"Z\",\n",
        "    \"a\",\n",
        "    \"b\",\n",
        "    \"c\",\n",
        "    \"d\",\n",
        "    \"e\",\n",
        "    \"f\",\n",
        "    \"g\",\n",
        "    \"h\",\n",
        "    \"i\",\n",
        "    \"j\",\n",
        "    \"k\",\n",
        "    \"l\",\n",
        "    \"m\",\n",
        "    \"n\",\n",
        "    \"o\",\n",
        "    \"p\",\n",
        "    \"q\",\n",
        "    \"r\",\n",
        "    \"s\",\n",
        "    \"t\",\n",
        "    \"u\",\n",
        "    \"v\",\n",
        "    \"w\",\n",
        "    \"x\",\n",
        "    \"y\",\n",
        "    \"z\",\n",
        "    \"ا\",\n",
        "    \"ب\",\n",
        "    \"ت\",\n",
        "    \"ث\",\n",
        "    \"ج\",\n",
        "    \"ح\",\n",
        "    \"خ\",\n",
        "    \"د\",\n",
        "    \"ذ\",\n",
        "    \"ر\",\n",
        "    \"ز\",\n",
        "    \"س\",\n",
        "    \"ش\",\n",
        "    \"ص\",\n",
        "    \"ض\",\n",
        "    \"ط\",\n",
        "    \"ظ\",\n",
        "    \"ع\",\n",
        "    \"غ\",\n",
        "    \"ف\",\n",
        "    \"ق\",\n",
        "    \"ل\",\n",
        "    \"م\",\n",
        "    \"ن\",\n",
        "    \"ه\",\n",
        "    \"و\",\n",
        "    \"پ\",\n",
        "    \"چ\",\n",
        "    \"ژ\",\n",
        "    \"ک\",\n",
        "    \"گ\",\n",
        "    \"ی\",\n",
        "]\n",
        "\n",
        "translation_table = dict((ord(a), b) for a, b in char_mappings.items())\n",
        "\n",
        "# Create a regex for recognizing invalid characters.\n",
        "nonvalid_reg_text = '[^{}]'.format(\"\".join(valid_chars))\n",
        "nonvalid_reg = re.compile(nonvalid_reg_text)\n",
        "\n",
        "\n",
        "def normalize_text(text, to_lower=True, remove_invalid=True):\n",
        "    # Map invalid characters with replacement to valid characters.\n",
        "    text = text.translate(translation_table)\n",
        "    if to_lower:\n",
        "        text = text.lower()\n",
        "    if remove_invalid:\n",
        "        text = nonvalid_reg.sub(' ', text)\n",
        "    # Replace consecutive whitespaces with a single space character.\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bf67bf42-1c22-416b-9b1b-40226db7fe04"
      },
      "outputs": [],
      "source": [
        "def read_json_lines(path, n_lines=None):\n",
        "    \"\"\"Creates a generator which reads and returns lines of\n",
        "    a json lines file, one line at a time, each as a dictionary.\n",
        "    \n",
        "    This could be used as a memory-efficient alternative of `pandas.read_json`\n",
        "    for reading a json lines file.\n",
        "    \"\"\"\n",
        "    with open(path, 'r') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if n_lines == i:\n",
        "                break\n",
        "            yield json.loads(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cc13ba98-ddc5-4092-afe9-847e209511b8"
      },
      "outputs": [],
      "source": [
        "class JSONLinesWriter:\n",
        "    \"\"\"\n",
        "    Helper class to write list of dictionaries into a file in json lines\n",
        "    format, i.e. one json record per line.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, file_path):\n",
        "        self.fd = None\n",
        "        self.file_path = file_path\n",
        "        self.delimiter = \"\\n\"\n",
        "\n",
        "    def open(self):\n",
        "        self.fd = open(self.file_path, \"w\")\n",
        "        self.first_record_written = False\n",
        "        return self\n",
        "\n",
        "    def close(self):\n",
        "        self.fd.close()\n",
        "        self.fd = None\n",
        "\n",
        "    def write_record(self, obj):\n",
        "        if self.first_record_written:\n",
        "            self.fd.write(self.delimiter)\n",
        "        self.fd.write(json.dumps(obj))\n",
        "        self.first_record_written = True\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self.open()\n",
        "\n",
        "    def __exit__(self, type, value, traceback):\n",
        "        self.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e576ecfa-12dd-474c-9041-70a4cd29fe4a"
      },
      "outputs": [],
      "source": [
        "data_dir = os.path.join('data')\n",
        "output_dir = os.path.join('output_data')\n",
        "\n",
        "search_data_path = os.path.join(data_dir, 'torob-search-data_v1.jsonl')\n",
        "aggregated_search_data_path = os.path.join(output_dir, 'aggregated_search_data.jsonl')\n",
        "\n",
        "products_path = os.path.join(data_dir, 'products-info_v1.jsonl')\n",
        "preprocessed_products_path = os.path.join(output_dir, 'preprocessed_products.jsonl')\n",
        "\n",
        "test_data_path = os.path.join(data_dir, 'test-offline-data_v1.jsonl')\n",
        "preprocessed_test_queries_path = os.path.join(output_dir, 'preprocessed_test_queries.jsonl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "986777b9-915c-4ca6-810d-59a75a39133b"
      },
      "outputs": [],
      "source": [
        "def aggregate_searches(search_data_path, output_path):\n",
        "    \"\"\"Aggregate searches based on raw query.\n",
        "    \n",
        "    For each unique raw query in the search data, the frequency of products and\n",
        "    clicked products would be aggregated.\n",
        "    \"\"\"\n",
        "    agg_searches = defaultdict(\n",
        "        lambda : dict(\n",
        "            results=Counter(),\n",
        "            clicks=Counter(),\n",
        "        )\n",
        "    )\n",
        "    print(\"Aggregating searches based on raw query...\")\n",
        "    for search in tqdm(read_json_lines(search_data_path)):\n",
        "        agg_searches[search['raw_query']]['results'].update(search['result'])\n",
        "        agg_searches[search['raw_query']]['clicks'].update(search['clicked_result'])\n",
        "    \n",
        "    print('Writing aggregated searches into file...')\n",
        "    with JSONLinesWriter(output_path) as out_file:\n",
        "        for raw_query, stats in tqdm(agg_searches.items()):\n",
        "            results, results_count = list(zip(*stats['results'].most_common()))\n",
        "            clicks, clicks_count = list(zip(*stats['clicks'].most_common()))\n",
        "            record = {\n",
        "                'raw_query': raw_query,\n",
        "                'raw_query_normalized': normalize_text(raw_query),\n",
        "                'results': results,\n",
        "                'results_count': results_count,\n",
        "                'clicks': clicks,\n",
        "                'clicks_count': clicks_count,\n",
        "            }\n",
        "            out_file.write_record(record)\n",
        "\n",
        "    print(\"Finished aggregating searches.\")\n",
        "    print(f'Number of aggregate search records: {len(agg_searches)}')\n",
        "    print(f\"The aggregated searches data were stored in '{output_path}'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8858f63-8a88-470e-82f7-eed6f600a354"
      },
      "outputs": [],
      "source": [
        "aggregate_searches(search_data_path, aggregated_search_data_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7cb1caa-c12f-4a66-8cb6-11e709c93d06"
      },
      "outputs": [],
      "source": [
        "def preprocess_products(products_path, output_path):\n",
        "    \"\"\"Preprocess product names.\n",
        "    \n",
        "    The different titles of a product are concatenated together and \n",
        "    the resulting string would be normalized. Then, the normalized title\n",
        "    is split into tokens and only the set of unique tokens would be selected\n",
        "    as the final title of the product.\n",
        "    \"\"\"\n",
        "    print('Preprocessing products...')\n",
        "    count = 0\n",
        "    with JSONLinesWriter(output_path) as out_file:\n",
        "        for product in tqdm(read_json_lines(products_path)):\n",
        "            titles = product['titles']\n",
        "            titles_concat_normalized = normalize_text(\" \".join(titles))\n",
        "            titles_words_set = set(titles_concat_normalized.split())\n",
        "            titles_words_concat = \" \".join(titles_words_set)\n",
        "            \n",
        "            record = {\n",
        "                'id': product['id'],\n",
        "                'title_normalized': titles_words_concat,\n",
        "            }\n",
        "            out_file.write_record(record)\n",
        "            count += 1\n",
        "    print('Finished preprocessing products.')\n",
        "    print(f'Number of processed products: {count}')\n",
        "    print(f\"The processed products data were stored in '{output_path}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9d38bf3a-5734-443b-bde8-db79c4c6945c"
      },
      "outputs": [],
      "source": [
        "preprocess_products(products_path, preprocessed_products_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0066cb1-6239-4c2b-8e26-6d1fc91f2f1d"
      },
      "outputs": [],
      "source": [
        "def preprocess_test_queries(test_data_path, output_path):\n",
        "    \"\"\"Normalize test queries.\"\"\"\n",
        "    print('Preprocessing test queries...')\n",
        "    count = 0\n",
        "    with JSONLinesWriter(output_path) as out_file:\n",
        "        for test_sample in tqdm(read_json_lines(test_data_path)):\n",
        "            normalized_query = normalize_text(test_sample['raw_query'])\n",
        "            record = {\n",
        "                'raw_query_normalized': normalized_query,\n",
        "            }\n",
        "            count += 1\n",
        "            out_file.write_record(record)\n",
        "    print('Finished preprocessing test queries.')\n",
        "    print(f'Number of processed test queries: {count}')\n",
        "    print(f\"The processed test queries were stored in '{output_path}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ff25fd27-d794-4652-9268-50ab34f2cc54"
      },
      "outputs": [],
      "source": [
        "preprocess_test_queries(test_data_path, preprocessed_test_queries_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d97fe91-f467-4b37-81fd-c702f4bb3018"
      },
      "source": [
        "## Feature extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6e134a2a-57eb-4d18-b9f3-3594f3e9700f"
      },
      "outputs": [],
      "source": [
        "# Reset environment due to memory constraints.\n",
        "%reset -f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dfa0661-ef2c-4f48-9469-299516b0c4c4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import gc\n",
        "import pickle\n",
        "\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bab25a9-f5fb-4498-869b-957ea0012631"
      },
      "outputs": [],
      "source": [
        "def read_json_lines(path, n_lines=None):\n",
        "    \"\"\"Creates a generator which reads and returns lines of\n",
        "    a json lines file, one line at a time, each as a dictionary.\n",
        "    \n",
        "    This could be used as a memory-efficient alternative of `pandas.read_json`\n",
        "    for reading a json lines file.\n",
        "    \"\"\"\n",
        "    with open(path, 'r') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if n_lines == i:\n",
        "                break\n",
        "            yield json.loads(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8b7b976c-c80b-4d61-9f35-e1342259ebf1"
      },
      "outputs": [],
      "source": [
        "output_dir = os.path.join('output_data')\n",
        "\n",
        "aggregated_search_data_path = os.path.join(output_dir, 'aggregated_search_data.jsonl')\n",
        "preprocessed_products_path = os.path.join(output_dir, 'preprocessed_products.jsonl')\n",
        "preprocessed_test_queries_path = os.path.join(output_dir, 'preprocessed_test_queries.jsonl')\n",
        "\n",
        "train_dat_file_path = os.path.join(output_dir, 'train.dat')\n",
        "\n",
        "random_projection_mat_path = os.path.join(output_dir, 'random_projection_mat.npy')\n",
        "product_features_path = os.path.join(output_dir, 'product_features.npy')\n",
        "queries_train_features_path = os.path.join(output_dir, 'queries_train_features.npy')\n",
        "queries_test_features_path = os.path.join(output_dir, 'queries_test_features.npy')\n",
        "products_id_to_idx_path = os.path.join(output_dir, 'products_id_to_idx.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0d1ba01e-828c-4bf7-90fe-de1c65ebe59d"
      },
      "outputs": [],
      "source": [
        "# Number of tokens in the vocabulary of TF-IDF.\n",
        "VOCAB_SIZE = 4096\n",
        "# Embedding dimension used for random projection of TF-IDF vectors.\n",
        "EMBEDDING_DIM = 256\n",
        "# Number of training samples to use (set to None to use all samples).\n",
        "NUM_TRAIN_SAMPLES = 10_000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2feaf72c-08b9-4d00-bdf1-c51779f55b63"
      },
      "outputs": [],
      "source": [
        "# Load aggregated search data which will be used as training data.\n",
        "aggregated_searches_df = pd.DataFrame(\n",
        "    read_json_lines(aggregated_search_data_path, n_lines=NUM_TRAIN_SAMPLES)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3fceb6e-494b-46ac-8448-0c1934ba52bd"
      },
      "outputs": [],
      "source": [
        "# Load preprocessed product data.\n",
        "products_data_df = pd.DataFrame(read_json_lines(preprocessed_products_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ae272eb5-6f51-45df-b892-945270d992cd"
      },
      "outputs": [],
      "source": [
        "# Load preprocessed test queries.\n",
        "test_offline_queries_df = pd.DataFrame(read_json_lines(preprocessed_test_queries_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bc4687bd-dc83-4150-88d2-1af4963248d8"
      },
      "outputs": [],
      "source": [
        "# Create a mapping from ID of products to their integer index.\n",
        "products_id_to_idx = dict(\n",
        "    (p_id, idx)\n",
        "    for idx, p_id in enumerate(products_data_df['id'])\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e6f7b31-b0d0-4ecf-945e-2960a7fedc6b"
      },
      "outputs": [],
      "source": [
        "# Create a random matrix which will be used for projection of\n",
        "# TF-IDF vector to a lower-ranked random space.\n",
        "random_projection_mat = np.random.rand(VOCAB_SIZE, EMBEDDING_DIM)\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=VOCAB_SIZE, lowercase=True, use_idf=True)\n",
        "\n",
        "# Fit tf-idf vectorizer on normalized product names and compute their tf-idf vectors.\n",
        "products_tfidf = vectorizer.fit_transform(products_data_df['title_normalized'])\n",
        "# Project the tf-idf vectors using random projection matrix.\n",
        "products_projected = products_tfidf.dot(random_projection_mat)\n",
        "del products_tfidf  # Free up memory.\n",
        "gc.collect()\n",
        "\n",
        "# Transform the training raw queries into tf-idf vectors.\n",
        "queries_train_tfidf = vectorizer.transform(aggregated_searches_df['raw_query_normalized'])\n",
        "queries_train_projected = queries_train_tfidf.dot(random_projection_mat)\n",
        "del queries_train_tfidf # Free up memory.\n",
        "gc.collect();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6c373800-c7a0-4b2f-8597-74448db1c190"
      },
      "outputs": [],
      "source": [
        "# Transform test raw queries into tf-idf vectors.\n",
        "queries_test_tfidf = vectorizer.transform(test_offline_queries_df['raw_query_normalized'])\n",
        "queries_test_projected = queries_test_tfidf.dot(random_projection_mat)\n",
        "del queries_test_tfidf # Free up memory.\n",
        "gc.collect();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5835a192-8047-4e3f-841e-9a01a5452567"
      },
      "outputs": [],
      "source": [
        "del vectorizer\n",
        "gc.collect();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12869561-8798-4cfd-acc2-0ace8e4374b7"
      },
      "source": [
        "For each pair of (Q, P) where Q is a search query and P is a product in the search-results of query Q, a training sample is generated. The features of Q and P will be their corresponding random-projected TF-IDF vectors. Further, the sample is labeled based on the number of clicks on P (in the search results of query Q)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76d5d81c-50b9-496e-a65c-d4bebb1d5e66"
      },
      "outputs": [],
      "source": [
        "def create_dat_file(\n",
        "    dat_file_path,\n",
        "    agg_searches_df,\n",
        "    query_features,\n",
        "    product_features,\n",
        "    n_candidates=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Create a `dat` file which is the training data of LambdaMart model.\n",
        "\n",
        "    The file format of the training and test files is the same as for SVMlight,\n",
        "    with the exception that the lines in the input files have to be sorted by increasing qid.\n",
        "    The first lines may contain comments and are ignored if they start with #.\n",
        "    Each of the following lines represents one training example and is of the following format:\n",
        "\n",
        "    <line> .=. <target> qid:<qid> <feature>:<value> <feature>:<value> ... <feature>:<value> # <info>\n",
        "    <target> .=. <float>\n",
        "    <qid> .=. <positive integer>\n",
        "    <feature> .=. <positive integer>\n",
        "    <value> .=. <float>\n",
        "    <info> .=. <string>\n",
        "\n",
        "    The target value and each of the feature/value pairs are separated by a space character.\n",
        "    Feature/value pairs MUST be ordered by increasing feature number.\n",
        "    Features with value zero can be skipped.\n",
        "    The target value defines the order of the examples for each query.\n",
        "    Implicitly, the target values are used to generated pairwise preference constraints as described in [Joachims, 2002c].\n",
        "    A preference constraint is included for all pairs of examples in the example_file, for which the target value differs.\n",
        "    The special feature \"qid\" can be used to restrict the generation of constraints.\n",
        "    Two examples are considered for a pairwise preference constraint only if the value of \"qid\" is the same.\n",
        "\n",
        "    For example, given the example_file\n",
        "\n",
        "    3 qid:1 1:1 2:1 3:0 4:0.2 5:0 # 1A\n",
        "    2 qid:1 1:0 2:0 3:1 4:0.1 5:1 # 1B\n",
        "    1 qid:1 1:0 2:1 3:0 4:0.4 5:0 # 1C\n",
        "    1 qid:1 1:0 2:0 3:1 4:0.3 5:0 # 1D\n",
        "    1 qid:2 1:0 2:0 3:1 4:0.2 5:0 # 2A\n",
        "    2 qid:2 1:1 2:0 3:1 4:0.4 5:0 # 2B\n",
        "    1 qid:2 1:0 2:0 3:1 4:0.1 5:0 # 2C\n",
        "    1 qid:2 1:0 2:0 3:1 4:0.2 5:0 # 2D\n",
        "    2 qid:3 1:0 2:0 3:1 4:0.1 5:1 # 3A\n",
        "    3 qid:3 1:1 2:1 3:0 4:0.3 5:0 # 3B\n",
        "    4 qid:3 1:1 2:0 3:0 4:0.4 5:1 # 3C\n",
        "    1 qid:3 1:0 2:1 3:1 4:0.5 5:0 # 3D\n",
        "\n",
        "    the following set of pairwise constraints is generated (examples are referred to by the info-string after the # character):\n",
        "\n",
        "    1A>1B, 1A>1C, 1A>1D, 1B>1C, 1B>1D, 2B>2A, 2B>2C, 2B>2D, 3C>3A, 3C>3B, 3C>3D, 3B>3A, 3B>3D, 3A>3D\n",
        "\n",
        "    More information:\n",
        "     - https://xgboost.readthedocs.io/en/latest/tutorials/input_format.html#embedding-additional-information-inside-libsvm-file\n",
        "     - https://www.cs.cornell.edu/people/tj/svm_light/svm_rank.html\n",
        "    \"\"\"\n",
        "    with open(dat_file_path, \"w\") as file:\n",
        "        for qid, agg_search in tqdm(enumerate(agg_searches_df.itertuples(index=False))):\n",
        "            if n_candidates is None:\n",
        "                limit = len(agg_search.results)\n",
        "            else:\n",
        "                limit = min(n_candidates, len(agg_search.results))\n",
        "            clicks = dict(zip(agg_search.clicks, agg_search.clicks_count))\n",
        "\n",
        "            for candidate_product_id in agg_search.results[:limit]:\n",
        "                if candidate_product_id is None:\n",
        "                    continue\n",
        "                candidate_score = clicks.get(candidate_product_id, 0)\n",
        "                candidate_score = np.log2(candidate_score + 1)\n",
        "\n",
        "                p_idx = products_id_to_idx[candidate_product_id]\n",
        "                features = np.concatenate((product_features[p_idx], query_features[qid]))\n",
        "                features = np.around(features, 3)\n",
        "\n",
        "                file.write(\n",
        "                    f\"{candidate_score} qid:{qid} \"\n",
        "                    + \" \".join([f\"{i}:{s}\" for i, s in enumerate(features)])\n",
        "                    + \"\\n\"\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "257d54d6-f5a9-4d96-a855-6bdd29a13310"
      },
      "outputs": [],
      "source": [
        "create_dat_file(\n",
        "    train_dat_file_path,\n",
        "    aggregated_searches_df,\n",
        "    queries_train_projected,\n",
        "    products_projected,\n",
        "    n_candidates=200,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b255ee09-6663-4db3-b6e6-07d65224b650",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Since memory is limited, we store all the neccessary data\n",
        "# such as extracted features on disk. Later, in inference\n",
        "# step we may need some of these files.\n",
        "np.save(random_projection_mat_path, random_projection_mat)\n",
        "np.save(product_features_path, products_projected)\n",
        "np.save(queries_train_features_path, queries_train_projected)\n",
        "np.save(queries_test_features_path, queries_test_projected)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2e036f5-0bc2-49db-8855-46100909db9e"
      },
      "outputs": [],
      "source": [
        "with open(products_id_to_idx_path, 'wb') as f:\n",
        "    pickle.dump(products_id_to_idx, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dc76429-3477-4161-888a-773bd1970edb"
      },
      "source": [
        "## Training of model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51a6a116-6989-420f-a510-118ecc73bb68"
      },
      "outputs": [],
      "source": [
        "# Reset due to memory constraints.\n",
        "%reset -f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9f1e0d3e-7ac1-415c-af9d-a08e07812fc5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import xgboost as xgb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e520007c-29f6-4546-b300-1018336ca427"
      },
      "outputs": [],
      "source": [
        "output_dir = os.path.join('output_data')\n",
        "\n",
        "train_dat_path = os.path.join(output_dir, 'train.dat')\n",
        "model_path = os.path.join(output_dir, 'ranker.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64777a2a-64fa-40f6-b1e5-8cd8ea21d778"
      },
      "outputs": [],
      "source": [
        "train_data = xgb.DMatrix(train_dat_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ac486bb-a07b-4e4d-9566-8660ff850d7e",
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "param = {\n",
        "    \"max_depth\": 20,\n",
        "    \"eta\": 0.3,\n",
        "    \"objective\": \"rank:ndcg\",\n",
        "    \"verbosity\": 1,\n",
        "    \"num_parallel_tree\": 1,\n",
        "    \"tree_method\": \"gpu_hist\",\n",
        "    \"eval_metric\": [\"ndcg\"],\n",
        "}\n",
        "eval_list = [(train_data, \"train\")]\n",
        "\n",
        "model = xgb.train(\n",
        "    param,\n",
        "    train_data,\n",
        "    num_boost_round=200,\n",
        "    evals=eval_list,\n",
        ")\n",
        "\n",
        "model.save_model(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70883ad7-eb07-40ed-bc8c-8fc3d2e1ab67"
      },
      "source": [
        "## Inference (prediction) on test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01689bb8-a6c5-44ff-9baf-28baad91b4f3"
      },
      "outputs": [],
      "source": [
        "# Reset due to memory constraints.\n",
        "%reset -f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aacd718-9b0b-42c0-b3bc-043782a877ba"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import json\n",
        "\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b40ebc88-9470-4a97-abeb-085ac10879d1"
      },
      "outputs": [],
      "source": [
        "def read_json_lines(path, n_lines=None):\n",
        "    \"\"\"Creates a generator which reads and returns lines of\n",
        "    a json lines file, one line at a time, each as a dictionary.\n",
        "    \n",
        "    This could be used as a memory-efficient alternative of `pandas.read_json`\n",
        "    for reading a json lines file.\n",
        "    \"\"\"\n",
        "    with open(path, 'r') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if n_lines == i:\n",
        "                break\n",
        "            yield json.loads(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31990863-d98f-4057-82a0-44a1660b1d3d"
      },
      "outputs": [],
      "source": [
        "data_dir = os.path.join('data')\n",
        "output_dir = os.path.join('output_data')\n",
        "\n",
        "test_data_path = os.path.join(data_dir, 'test-offline-data_v1.jsonl')\n",
        "\n",
        "product_features_path = os.path.join(output_dir, 'product_features.npy')\n",
        "queries_test_features_path = os.path.join(output_dir, 'queries_test_features.npy')\n",
        "products_id_to_idx_path = os.path.join(output_dir, 'products_id_to_idx.pkl')\n",
        "\n",
        "predictions_path = os.path.join(output_dir, 'predictions.txt')\n",
        "\n",
        "model_path = os.path.join(output_dir, 'ranker.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1e3cfe67-e69d-46ad-877d-98626e449231"
      },
      "outputs": [],
      "source": [
        "# Load projected products and queries data.\n",
        "products_projected = np.load(product_features_path)\n",
        "queries_test_projected = np.load(queries_test_features_path)\n",
        "with open(products_id_to_idx_path, 'rb') as f:\n",
        "    products_id_to_idx = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84c18749-f799-46de-bd6c-3ed3713c561f"
      },
      "outputs": [],
      "source": [
        "# Load original test data which contains the result to be ranked.\n",
        "test_data_df = pd.DataFrame(read_json_lines(test_data_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eca8249-dbfd-42b1-864f-2d26e037a68c"
      },
      "outputs": [],
      "source": [
        "# Load trained LambdaMART model.\n",
        "param = {}\n",
        "model = xgb.Booster(**param)\n",
        "model.load_model(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hz_nb5J79Sz9"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "test_predictions = []\n",
        "for batch_idx in tqdm(range(0, len(test_data_df), BATCH_SIZE)):\n",
        "    batch_data = test_data_df['result_not_ranked'].iloc[batch_idx:batch_idx + BATCH_SIZE]\n",
        "    batch_features = []\n",
        "    for test_qid, test_candidates in enumerate(batch_data, start=batch_idx):\n",
        "        test_query_projected = queries_test_projected[test_qid]\n",
        "        for candidate_pid in test_candidates:\n",
        "            p_idx = products_id_to_idx[candidate_pid]\n",
        "            features = np.concatenate((products_projected[p_idx], test_query_projected))\n",
        "            batch_features.append(features)\n",
        "    \n",
        "    batch_features = np.stack(batch_features)\n",
        "    batch_features = xgb.DMatrix(batch_features)\n",
        "    batch_preds = model.predict(batch_features)\n",
        "    \n",
        "    start_idx = 0\n",
        "    for test_candidates in batch_data:\n",
        "        preds_sample = batch_preds[start_idx:start_idx + len(test_candidates)]\n",
        "        sorted_idx = np.argsort(preds_sample)[::-1]\n",
        "        sorted_candidates = [test_candidates[i] for i in sorted_idx]\n",
        "        test_predictions.append(sorted_candidates)\n",
        "        start_idx += len(test_candidates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33637f50-f4db-48ad-b42a-d5ebe06ccbe1"
      },
      "outputs": [],
      "source": [
        "def write_test_predictions(predictions_path, predictions):\n",
        "    lines = []\n",
        "    for preds in predictions:\n",
        "        lines.append(\",\".join([str(p_id) for p_id in preds]))\n",
        "\n",
        "    with open(predictions_path, 'w') as f:\n",
        "        f.write(\"\\n\".join(lines))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "da5bab12-7474-470a-a8bd-157e83f7a727"
      },
      "outputs": [],
      "source": [
        "write_test_predictions(predictions_path, test_predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12BroI_rHEUn"
      },
      "source": [
        "Now, you can submit the `predictions.txt` file in the `output_data` directory using RoboEpics platform."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tensorflow",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "921c1c4932ebc6534d21dcf2780023961ea40e3e28695ae20f3dbb7777514cf0"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
